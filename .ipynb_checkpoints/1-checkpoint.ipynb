{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------  Section 2  ------------------------------\n",
    "### Classical vs Deep Learning Models\n",
    "- Some examples :\n",
    "1. If-Else Rule based chatbot\n",
    "2. Speech Recog\n",
    "3. Bag of words model : Classification\n",
    "4. CNN for text Recognition\n",
    "\n",
    "### End to end deep learning\n",
    "- suppose 2 models are there, 1 for converrting speech to text and another for analyzing the text. So error can increase\n",
    "- Soln : end to end deep learning. ie. 1 model for whole thing\n",
    "    - ie. seq to seq is end to end deep learning model\n",
    "    \n",
    "### Seq2Seq Architecture\n",
    "- Issues with bag of words model\n",
    "    1. fixed sized inputs and outputs\n",
    "    2. does not consider ordering of words\n",
    "\n",
    "- SOLN : RNNs \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Architecture\n",
    "- We have a dense vector correspoding where we have start of sentence, end of sentence and a value correspoding to each word in our sentence.\n",
    "- ex. length of sentence 8, .'. length of vector corresponding to the sentence = SOS + 8 + EOS = 10 \n",
    "    - value of SOS = 1 always and EOS = 2\n",
    "    - every word have a value, if word is two places we can see same value two places\n",
    "    - We can remove SOS as we know starting sentence. EOS is imp as it tells when the output will terminate.\n",
    "    \n",
    "- As we see end of sentence, we start generating output. \n",
    "\n",
    "- So we have an enocder part and a decoder part.\n",
    "- We can have deep networks as well\n",
    "\n",
    "### Training\n",
    "- ex. input : Did you like that EOS\n",
    "    - output : Yes it was great EOS\n",
    "- In Seq2Seq, in decoder part we are passing the output of previous time step to the next time step as input\n",
    "\n",
    "\n",
    "- Q. : How can it adapt to different input and output lengths for different examples ? \n",
    "- Ans : Encoder part we have single weight w1, decoder we have single weight w2. Is is a time step, so time steps can change but weight will be same\n",
    "\n",
    "### Beam Search Decoding\n",
    "1. Greedy Decoding : y<1> word that has highest probab is fed to next time step in decoder and thus we get y<2> that has highes prob. This continues till we get EOS.\n",
    "    - Greedy because we look at the word with highest probability\n",
    "    \n",
    "2. Beam Search Decoding:\n",
    "    - here we will look at top n probability words, ex. top 3 or top 10. ie. 3 beams or 10 beams.\n",
    "    - Now we have three versions of seq2seq, one version with word as yes, another with I'm and another with Thanks.\n",
    "    - now same for each of the three, another three seq2seq will be produced. This is TREE like structure.\n",
    "    - Thus we choose a combination or beam which has the maximum joint probability. \n",
    "    - NOTE: beam grows quickly. 1st time 3, 2nd time 9 ...\n",
    "        - Soln : Truncating the beams : \n",
    "            - if joint probab starts going low, it will throw the beam.\n",
    "    - There are also techniques for variability ie. all answers are not similar.\n",
    "    \n",
    "\n",
    "### Attention Mechanism\n",
    "- In Seq2Seq model we have an encoder LSTM and a decoder LSTM.\n",
    "    - at the last input time step ie. EOS step, we have the representation of whole input which is the meaning of our sentence.\n",
    "    - decoder will take this and gives us some response\n",
    "    - this is weak point of this architecture. We are having memory but also stacking up the meaning to the end time step.\n",
    "    - now this is a fixed dimensional representation but input can be of variable lengths .'. It becomes a lot of info to store if input becomes large\n",
    "    - Now this representation or meaning will be taken by the decoder and it should be able to maintain all the info in the layer.\n",
    "    - This approach is OK for short sentences and short responses\n",
    "\n",
    "- Soln: Additional to the representation, our decoder should have access to previous input timestep additional to the last one \n",
    "    - now with learning for each word we get weights. We will have a Context Vector which is weghted sum of all these layers. \n",
    "    - ie. w1*a1 + w2*a2 + w3*a3 { suppose we had 3 word input } \n",
    "    - w1,w2,w3 are for diff timesteps\n",
    "    - Now we will feed this context vector as an additional layer to decoder as input.\n",
    "    \n",
    "#### Global vs Local attention\n",
    "- This was global attention, additionally we have Local attention. \n",
    "- In global attention, we take the all the words in the input and add the weighted sum to the context vector, but in case of local attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
